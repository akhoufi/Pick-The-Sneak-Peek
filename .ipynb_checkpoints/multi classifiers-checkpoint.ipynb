{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriel\\Anaconda3\\envs\\py2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk import *\n",
    "from sklearn import tree\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, hamming_loss\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random\n",
    "from scipy.stats import randint as sp_randint\n",
    "import scipy.sparse as sp\n",
    "# from autosklearn.classification import AutoSklearnClassifier\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def pickle_file(filename, obj):\n",
    "    with open('dumps/' + filename, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def unpickle_file(filename):\n",
    "    with open('dumps/' + filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def unpickle_ref(filename):\n",
    "    with open('ref/' + filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def preprocess(text):\n",
    "# Remove punctuation, stopword and then stemmering\n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "    stemmer = stem.PorterStemmer()\n",
    "    punctuation = set(string.punctuation)\n",
    "\n",
    "    doc = [stemmer.stem(word) for word in nltk.word_tokenize(text) if (word not in punctuation) and (word not in stop)]\n",
    "\n",
    "    doc = ' '.join(w for w in doc)\n",
    "    return doc\n",
    "\n",
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    '''\n",
    "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
    "    http://stackoverflow.com/q/32239577/395857\n",
    "    '''\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        #print('tmp_a: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info file NOT found : D:\\Documents\\Pick-The-Sneak-Peek\\ref\\movies_public.info\n",
      "Reading ref\\movies_train.data...\n",
      "Converting {} to correct array\n",
      "Converting ref\\movies_train.data to sparse list\n",
      "Reading ref\\movies_test.data...\n",
      "Converting {} to correct array\n",
      "Converting ref\\movies_test.data to sparse list\n",
      "Reading ref\\movies_valid.data...\n",
      "Converting {} to correct array\n",
      "Converting ref\\movies_valid.data to sparse list\n",
      "Reading ref/movies_train.data...\n",
      "Converting {} to correct array\n",
      "Converting ref/movies_train.data to sparse list\n",
      "\tConverting sparse list to dok sparse matrix\n",
      "\tConverting dok sparse matrix to csr sparse matrix\n",
      "Reading ref/movies_valid.data...\n",
      "Converting {} to correct array\n",
      "Converting ref/movies_valid.data to sparse list\n",
      "\tConverting sparse list to dok sparse matrix\n",
      "\tConverting dok sparse matrix to csr sparse matrix\n",
      "Reading ref/movies_test.data...\n",
      "Converting {} to correct array\n",
      "Converting ref/movies_test.data to sparse list\n",
      "\tConverting sparse list to dok sparse matrix\n",
      "\tConverting dok sparse matrix to csr sparse matrix\n"
     ]
    }
   ],
   "source": [
    "# Lecture des dataset\n",
    "import os\n",
    "from sys import argv, path\n",
    "run_dir = os.path.abspath(\".\")\n",
    "lib_dir = os.path.join(run_dir, \"sample_code\")\n",
    "res_dir = os.path.join(run_dir, \"res\")\n",
    "path.append (run_dir)\n",
    "path.append (lib_dir)\n",
    "import data_io                      \n",
    "from data_io import vprint           \n",
    "from data_manager import DataManager \n",
    "import data_io                      \n",
    "from data_io import vprint           \n",
    "from data_manager import DataManager \n",
    "from data_io import vprint           \n",
    "from data_manager import DataManager \n",
    "from classifier import Classifier     \n",
    "\n",
    "max_samples = 50000\n",
    "\n",
    "root_dir = \"\" # Changer le path\n",
    "default_input_dir = root_dir + \"ref/\"\n",
    "default_output_dir = root_dir + \"scoring_input_1_2/res\"\n",
    "\n",
    "input_dir = default_input_dir\n",
    "output_dir = default_output_dir\n",
    "\n",
    "datanames = data_io.inventory_data(input_dir)\n",
    "\n",
    "for basename in datanames:\n",
    "    D = DataManager(basename, input_dir, replace_missing=True, filter_features=True, max_samples=max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = D.data['X_train']\n",
    "y_train = D.data['Y_train']\n",
    "X_valid = D.data['X_valid']\n",
    "X_test = D.data['X_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_linear_svc(features_train, labels_train):\n",
    "    print (\"Training linear SVC\")\n",
    "    clf = SVC(C=1, class_weight='balanced')\n",
    "    clf = OneVsRestClassifier(clf, n_jobs=1).fit(features_train, labels_train)\n",
    "    print(\"ending fitting\")\n",
    "    return clf\n",
    "\n",
    "\n",
    "def train_randomForest(features_train, labels_train):\n",
    "    print (\"Training random forest\")\n",
    "    clf = RandomForestClassifier(n_estimators=70, max_depth=70)\n",
    "    clf = OneVsRestClassifier(clf, n_jobs=3).fit(features_train, labels_train)\n",
    "    return clf\n",
    "\n",
    "def train_decisionTree(features_train, labels_train):\n",
    "    print (\"Training decision tree\")\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = OneVsRestClassifier(clf, n_jobs=3).fit(features_train, labels_train)\n",
    "    return clf\n",
    "\n",
    "def train_adaboost_decision_tree(features_train, labels_train):\n",
    "    print (\"Training adaboost decision tree\")\n",
    "    clf = AdaBoostClassifier( tree.DecisionTreeClassifier(max_depth=5),\n",
    "                              n_estimators=600,\n",
    "                              learning_rate=1)\n",
    "    clf = OneVsRestClassifier(clf).fit(features_train, labels_train)\n",
    "    return clf\n",
    "\n",
    "def train_autosklearn(features_train, labels_train):\n",
    "    print (\"Training automl\")\n",
    "    clf = autosklearn.classification.AutoSklearnClassifier()\n",
    "    clf = OneVsRestClassifier(clf).fit(features_train, labels_train)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def train_SGD_l1(features_train, labels_train):\n",
    "    print (\"Training SGD l1\")\n",
    "    clf = SGDClassifier(alpha=.0001, n_iter=50, penalty=\"l1\", loss=\"log\")\n",
    "    clf = OneVsRestClassifier(clf, n_jobs=3).fit(features_train, labels_train)\n",
    "    return clf\n",
    "                        \n",
    "def train_SGD_l2(features_train, labels_train):\n",
    "    print (\"Training SGD l2\")\n",
    "    clf = SGDClassifier(alpha=.0001, n_iter=50, penalty=\"l2\")\n",
    "    clf = OneVsRestClassifier(clf, n_jobs=3).fit(features_train, labels_train)\n",
    "    return clf\n",
    "\n",
    "def train_SGD_elasticnet(features_train, labels_train):\n",
    "    print (\"Training SGD elascticnet\")\n",
    "    clf = SGDClassifier(alpha=.0001, n_iter=50, penalty=\"elasticnet\")\n",
    "    clf = OneVsRestClassifier(clf, n_jobs=3).fit(features_train, labels_train)\n",
    "    return clf\n",
    "\n",
    "def train_PassiveAggressiveClassifier(features_train, labels_train):\n",
    "    print (\"Training PassiveAggressiveClassifier\")\n",
    "    clf = PassiveAggressiveClassifier(n_iter=50)\n",
    "    clf = OneVsRestClassifier(clf, n_jobs=3).fit(features_train, labels_train)\n",
    "    return clf\n",
    "\n",
    "def train_RidgeClassifier(features_train, labels_train):\n",
    "    print (\"Training RidgeClassifier\")\n",
    "    clf = RidgeClassifier(tol=1e-2, solver=\"sag\")\n",
    "    clf = OneVsRestClassifier(clf, n_jobs=3).fit(features_train, labels_train)\n",
    "    return clf\n",
    "\n",
    "def train_KNeighborsClassifier(features_train, labels_train):\n",
    "    print (\"Training KNeighborsClassifier\")\n",
    "    clf = KNeighborsClassifier(n_neighbors=10)\n",
    "    clf = OneVsRestClassifier(clf).fit(features_train, labels_train)\n",
    "    return clf\n",
    "\n",
    "def train_MultinomialNB(features_train, labels_train):\n",
    "    print (\"Training MultinomialNB\")\n",
    "    clf = MultinomialNB(alpha=.01)\n",
    "    clf = OneVsRestClassifier(clf, n_jobs=3).fit(features_train, labels_train)\n",
    "    return clf\n",
    "\n",
    "def train_BernoulliNB(features_train, labels_train):\n",
    "    print (\"Training BernoulliNB\")\n",
    "    clf = BernoulliNB(alpha=.005)\n",
    "    clf = OneVsRestClassifier(clf, n_jobs=3).fit(features_train, labels_train)\n",
    "    return clf\n",
    "\n",
    "def train_MLP(features_train, labels_train):\n",
    "    print (\"Training MLP\")\n",
    "    clf = MLPClassifier(random_state=0, max_iter=400)\n",
    "    clf = OneVsRestClassifier(clf, n_jobs=3).fit(features_train, labels_train)\n",
    "    return clf\n",
    "\n",
    "def train_NearestCentroid(features_train, labels_train):\n",
    "    print (\"Training NearestCentroid\")\n",
    "    clf = NearestCentroid()\n",
    "    clf = OneVsRestClassifier(clf).fit(features_train, labels_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14823  5120  4603  1872  3848  1658  2059  1600  5003  1762  5217  1181\n",
      "   798  2429]\n",
      "[2019  687  606  246  530  198  276  213  679  233  704  160   94  312]\n",
      "[2990 1001  892  364  768  325  424  306  984  347  986  224  158  478]\n"
     ]
    }
   ],
   "source": [
    "y_valid = np.loadtxt(\"ref/movies_valid.solution\")\n",
    "y_test = np.loadtxt(\"ref/movies_test.solution\")\n",
    "print(np.count_nonzero(y_train, axis=0))\n",
    "print(np.count_nonzero(y_valid, axis=0))\n",
    "print(np.count_nonzero(y_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_roc_tab = []\n",
    "\n",
    "target_names = [\"drama\", \"action\", \"adventure\", \"animation\", \"crime\", \"fantasy\", \"music\", \"mystery\", \"romance\",\n",
    "                \"science fiction\", \"thriller\", \"war\", \"western\", \"family film\"]\n",
    "\n",
    "def benchmark(clf, name):  \n",
    "    t0 = time() \n",
    "    if name == 'xgboost':\n",
    "        pred = clf.predict(sp.hstack((X_test, sp.csr_matrix(np.ones((X_test.shape[0],1))))))\n",
    "    else:\n",
    "        pred = clf.predict(X_test)\n",
    "        # pred_proba = clf.predict_proba(X_test)\n",
    "    \n",
    "#     print(pred)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "    \n",
    "    score_hamming = hamming_score(y_test, pred)    \n",
    "    f1_micro = f1_score(y_test, pred, average=\"micro\")\n",
    "    f1_macro = f1_score(y_test, pred, average=\"macro\")\n",
    "    f1_weighted = f1_score(y_test, pred, average=\"weighted\")\n",
    "    f1_samples = f1_score(y_test, pred, average=\"samples\")\n",
    "    \n",
    "    print('F1-Score with micro compute: {0}'.format(f1_micro))\n",
    "    print('F1-Score with macro compute: {0}'.format(f1_macro))\n",
    "    print('F1-Score with weighted compute: {0}'.format(f1_weighted))\n",
    "    print('F1-Score with samples compute: {0}'.format(f1_samples))\n",
    "    print('Hamming score: {0}'.format(score_hamming))\n",
    "    print \"\\n\"\n",
    "    print classification_report(y_test, pred, target_names=target_names)\n",
    "    \n",
    "    # draw_roc_tab.append((ydf_test, pred_proba, name))\n",
    "    return name, score_hamming, f1_micro, f1_macro, f1_weighted, f1_samples\n",
    "\n",
    "\n",
    "\n",
    "def draw_ROC(y_real_and_y_pedit_and_clf_name_list):\n",
    "    \"\"\" y_test_and_y_score: une liste de y_test et y_score.\n",
    "        [(y_test, y_score, name), (y_test, y_score, name),...]\n",
    "        y_test est label réal\n",
    "        y_score est label prédit \n",
    "        name est le nom de classification\n",
    "       \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from itertools import cycle\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from scipy import interp\n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    for y_test, y_score, name in y_real_and_y_pedit_and_clf_name_list:\n",
    "        n_classes = y_score.shape[1]\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        # Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        # Compute macro-average ROC curve and ROC area\n",
    "        # First aggregate all false positive rates\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "        # Then interpolate all ROC curves at this points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= n_classes\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                 label = name + ' micro ROC (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"micro\"]),  linewidth=2)\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                 label = name + ' macro ROC (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"macro\"]),  linewidth=2)\n",
    "\n",
    "        #for i in range(n_classes):\n",
    "        #    plt.plot(fpr[i], tpr[i], lw=2,\n",
    "        #    label='ROC curve of {0} (area = {1:0.2f})'\n",
    "        #             ''.format(data.columns.values[i+2], roc_auc[i]))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic to multi-labels')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_classifier_linear = train_linear_svc(X_train, y_train)\n",
    "pickle_file('svm_classifier_linear', svm_classifier_linear)\n",
    "# svm_classifier_linear = unpickle_file('svm_classifier_linear')\n",
    "results.append(benchmark(svm_classifier_linear, 'svm_classifier_linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training random forest\n"
     ]
    }
   ],
   "source": [
    "random_forest = train_randomForest(X_train, y_train)\n",
    "pickle_file('random_forest', random_forest)\n",
    "# random_forest = unpickle_file('random_forest')\n",
    "results.append(benchmark(random_forest, 'random_forest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decision_tree = train_decisionTree(X_train, y_train)\n",
    "pickle_file('decision_tree', decision_tree)\n",
    "# decision_tree = unpickle_file('decision_tree')\n",
    "results.append(benchmark(decision_tree, 'decision_tree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adaboost_decision_tree = train_adaboost_decision_tree(X_train, y_train)\n",
    "pickle_file('adaboost_decision_tree', adaboost_decision_tree)\n",
    "# adaboost_decision_tree = unpickle_file('adaboost_decision_tree')\n",
    "results.append(benchmark(adaboost_decision_tree, 'adaboost_decision_tree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SGD_l1 = train_SGD_l1(X_train, y_train)\n",
    "# pickle_file('SGD_l1.dat', SGD_l1)\n",
    "SGD_l1 = unpickle_file('SGD_l1.dat')\n",
    "results.append(benchmark(SGD_l1, 'SGD_l1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SGD_l2 = train_SGD_l2(X_train, y_train)\n",
    "# pickle_file('SGD_l2', SGD_l2)\n",
    "SGD_l2 = unpickle_file('SGD_l2')\n",
    "results.append(benchmark(SGD_l2, 'SGD_l2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SGD_elasticnet = train_SGD_elasticnet(X_train, y_train)\n",
    "# pickle_file('SGD_elasticnet', SGD_elasticnet)\n",
    "SGD_elasticnet = unpickle_file('SGD_elasticnet')\n",
    "results.append(benchmark(SGD_elasticnet, 'SGD_elasticnet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PassiveAggressiveClassifier = train_PassiveAggressiveClassifier(X_train, y_train)\n",
    "# pickle_file('PassiveAggressiveClassifier', PassiveAggressiveClassifier)\n",
    "PassiveAggressiveClassifier = unpickle_file('PassiveAggressiveClassifier')\n",
    "results.append(benchmark(PassiveAggressiveClassifier, 'PassiveAggressiveClassifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RidgeClassifier = train_RidgeClassifier(X_train, y_train)\n",
    "# pickle_file('RidgeClassifier', RidgeClassifier)\n",
    "RidgeClassifier = unpickle_file('RidgeClassifier')\n",
    "results.append(benchmark(RidgeClassifier, 'RidgeClassifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KNeighborsClassifier = train_KNeighborsClassifier(X_train, y_train)\n",
    "pickle_file('KNeighborsClassifier', KNeighborsClassifier)\n",
    "# KNeighborsClassifier = unpickle_file('KNeighborsClassifier')\n",
    "results.append(benchmark(KNeighborsClassifier, 'KNeighborsClassifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MultinomialNB = train_MultinomialNB(X_train, y_train)\n",
    "# pickle_file('MultinomialNB', MultinomialNB)\n",
    "MultinomialNB = unpickle_file('MultinomialNB')\n",
    "results.append(benchmark(MultinomialNB, 'MultinomialNB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BernoulliNB = train_BernoulliNB(X_train, y_train)\n",
    "# pickle_file('BernoulliNB', BernoulliNB)\n",
    "BernoulliNB = unpickle_file('BernoulliNB')\n",
    "results.append(benchmark(BernoulliNB, 'BernoulliNB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MLP = train_MLP(X_train, y_train)\n",
    "pickle_file('MLP', MLP)\n",
    "# MLP = unpickle_file('MLP')\n",
    "results.append(benchmark(MLP, 'MLP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NearestCentroid = train_NearestCentroid(X_train, y_train)\n",
    "pickle_file('NearestCentroid', NearestCentroid)\n",
    "results.append(benchmark(NearestCentroid, 'NearestCentroid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_roc_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_ROC(draw_roc_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import f1_score\n",
    "score = []\n",
    "for i in range(500):\n",
    "    X_sub_test, y_sub_test = resample(X_test, ydf_test,replace=True)\n",
    "    y_pred = SGD_l1.predict(X_sub_test)\n",
    "    f1_weighted = f1_score(y_sub_test, y_pred, average = \"weighted\")\n",
    "    print(f1_weighted)\n",
    "    score.append(f1_weighted)\n",
    "print np.std(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "indices = np.arange(len(results))\n",
    "\n",
    "results_final = [[x[i] for x in results] for i in range(6)]\n",
    "width = 0.18  \n",
    "clf_names, score_hamming,f1_micro, f1_macro, f1_weighted, f1_samples = results_final\n",
    "# test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, score_hamming, .2, label=\"score hamming\", color='red')\n",
    "plt.barh(indices + width, f1_micro, .2, label=\"score F1 micro\", color='green')\n",
    "plt.barh(indices + 2*width, f1_macro, .2, label=\"score F1 macro\", color='blue')\n",
    "plt.barh(indices + 3*width, f1_weighted, .2, label=\"score F1 weighted\", color='orange')\n",
    "plt.barh(indices + 4*width, f1_samples, .2, label=\"score F1 samples\", color='yellow')\n",
    "\n",
    "plt.yticks(())\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
